{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXxoV32N-Fq5"
      },
      "source": [
        "# 📝 Text Generation Project — GPT-2 & LSTM\n",
        "\n",
        "This notebook demonstrates text generation using:\n",
        "- **GPT-2 (transformers)** for modern transformer-based generation\n",
        "- **A simple LSTM** (Keras) for educational sequence modeling\n",
        "\n",
        "---"
      ],
      "id": "TXxoV32N-Fq5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWre3F6l-Fq6",
        "outputId": "de0a8ec0-39cf-46ab-e55f-65470ec4b7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'markupsafe' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'markupsafe'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for markupsafe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "werkzeug 3.1.3 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.\n",
            "flask 3.1.2 requires markupsafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 🚀 Environment setup (auto-fix dependencies)\n",
        "# This cell makes sure you have the correct versions of packages.\n",
        "# Run this once at the start of your notebook.\n",
        "\n",
        "!pip install --quiet --upgrade pip\n",
        "\n",
        "# Core dependencies\n",
        "!pip install --quiet torch transformers accelerate\n",
        "\n",
        "# Web UI / Playground\n",
        "!pip install --quiet gradio\n",
        "\n",
        "# Fix compatibility issues\n",
        "!pip install --quiet \"markupsafe==2.0.1\" \"typing-extensions>=4.8.0\" \"urllib3>=2.0\" \"packaging>=24.0\" \"fsspec>=2023.5.0\"\n"
      ],
      "id": "LWre3F6l-Fq6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1BS4x5R-Fq7"
      },
      "source": [
        "## 🚀 GPT-2 Demo\n",
        "Load a pretrained GPT-2 and generate text from prompts."
      ],
      "id": "u1BS4x5R-Fq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ49Ev-V-Fq8",
        "outputId": "5d21c288-68ce-4ed4-cc62-5617f4267003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=400) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "internship program. The program is intended to create the capacity to have a strong and effective leadership role within the community, to encourage collaboration, to develop partnerships, and to foster mutual understanding.\n",
            "\n",
            "The students receive an education in English, History, Politics, and Literature, and are expected to complete their coursework in the second year. They will be responsible for meeting the academic standards of the program, which include the requirement for a bachelor's degree, master's degree, or equivalent in English from a university in which they are enrolled.\n",
            "\n",
            "The students will also be expected to complete a master's degree in the Department of Economics, with the goal of completing their studies in economics. They will be expected to complete an equivalency degree and complete their master's degree in the Department of Economics.\n",
            "\n",
            "The program is intended to provide a strong and effective leadership role within the community, to encourage collaboration, to develop partnerships, and to foster mutual understanding.\n",
            "\n",
            "Students will be provided with a place to work and participate in community organizations, such as local community organizations, community organizations for youth, local communities and organizations for youth with disabilities. The students will be able to participate in public or private civic activities, as well as in the administration of the program.\n",
            "\n",
            "A second year\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Device setup\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "MODEL_NAME = 'gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Pipeline for generation\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "def generate_gpt2(prompt, max_length=200, temperature=0.8, top_p=0.9, top_k=50, num_return_sequences=1):\n",
        "    out = gen(prompt, max_length=max_length, temperature=temperature, top_p=top_p, top_k=top_k, num_return_sequences=num_return_sequences)\n",
        "    return [o['generated_text'] for o in out]\n",
        "\n",
        "# 🔥 Test GPT-2 generation\n",
        "print(generate_gpt2('internship', max_length=400)[0])\n"
      ],
      "id": "qZ49Ev-V-Fq8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvc0u6HW-Fq8"
      },
      "source": [
        "## 🔤 LSTM Demo (toy)\n",
        "A very small word-level LSTM trained on a tiny sample text for demonstration."
      ],
      "id": "vvc0u6HW-Fq8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSB768P9-Fq8",
        "outputId": "e0374aa7-d761-4f9c-c628-2a18c897342b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artificial intelligence is changing people people interact with technology. build build models that can translate, translate, and understand languages. models models are trained on\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "sample_text = \"\"\"Artificial intelligence is changing how people interact with technology.\n",
        "Researchers build models that can write, translate, and understand languages.\n",
        "These models are trained on large datasets and require compute resources.\"\"\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = sample_text.lower().split()\n",
        "vocab = sorted(set(tokens))\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "# Build sequences\n",
        "seq_length = 5\n",
        "sequences = []\n",
        "next_words = []\n",
        "for i in range(len(tokens) - seq_length):\n",
        "    sequences.append([word2idx[w] for w in tokens[i:i+seq_length]])\n",
        "    next_words.append(word2idx[tokens[i+seq_length]])\n",
        "\n",
        "X = np.array(sequences)\n",
        "y = keras.utils.to_categorical(next_words, num_classes=len(vocab))\n",
        "\n",
        "# Define model\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(input_dim=len(vocab), output_dim=16, input_length=seq_length),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dense(len(vocab), activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Sampling function\n",
        "def sample_from_model(seed_text, gen_len=20):\n",
        "    words = seed_text.lower().split()\n",
        "    words = words[:seq_length]\n",
        "    for _ in range(gen_len):\n",
        "        seq = [word2idx.get(w,0) for w in words[-seq_length:]]\n",
        "        pred = model.predict(np.array([seq]), verbose=0)[0]\n",
        "        ix = np.argmax(pred)\n",
        "        words.append(idx2word[ix])\n",
        "    return ' '.join(words)\n",
        "\n",
        "# 🔥 Test LSTM\n",
        "print(sample_from_model('artificial intelligence is changing'))\n"
      ],
      "id": "TSB768P9-Fq8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfEUqhYH-Fq9"
      },
      "outputs": [],
      "source": [],
      "id": "DfEUqhYH-Fq9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbKEyveD-Fq9"
      },
      "outputs": [],
      "source": [],
      "id": "gbKEyveD-Fq9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeuaTyqK-Fq9"
      },
      "outputs": [],
      "source": [],
      "id": "XeuaTyqK-Fq9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz3Y2LZn-Fq9"
      },
      "outputs": [],
      "source": [],
      "id": "Yz3Y2LZn-Fq9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_soHEWe-Fq9"
      },
      "outputs": [],
      "source": [],
      "id": "q_soHEWe-Fq9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}